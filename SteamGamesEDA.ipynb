{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Custom Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_list_like(list_like_series:pd.core.series.Series, asType:str) -> pd.core.series.Series:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    list_like_series - Pandas Series in a \"list-like\" format as string\n",
    "    asType - String type to return values as using Pandas' .astype() method\n",
    "    Outputs:\n",
    "    unpacked_series - The unpacked Series returned as a \"/\" seperated string\n",
    "    Description:\n",
    "    This function takes list-like string values and unpacks them, removing\n",
    "    brackets and commas and replacing them with \"/\" for easier splitting\n",
    "    into usable Python native lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove open and close square brackets, single-quotes, and replace commas with forward slashes\n",
    "    unpacked_series = (list_like_series.str.replace(\"[\",\"\")\n",
    "                       .str.replace(\"]\",\"\")\n",
    "                       .str.replace(\"', '\",\"/\")\n",
    "                       .str.replace(\"\\'\",\"\"))\n",
    "    \n",
    "    return unpacked_series.astype(asType)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Section 1: Load and Initial Assesment\n",
    "In this section, the DataFrame is loaded in raw format in two zipped parts, and concatenated. The method `.info()` of the DataFrame class is used to gather initial insights about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the dataset parts into DataFrames and concatenate them into a single DataFrame\n",
    "games_sub1 : pd.core.frame.DataFrame = pd.read_csv(\"datasets/games_may2024_cleaned_1of2.zip\", encoding='latin1', low_memory=False)\n",
    "games_sub2 : pd.core.frame.DataFrame = pd.read_csv(\"datasets/games_may2024_cleaned_2of2.zip\", encoding='latin1', low_memory=False)\n",
    "\n",
    "games_raw : pd.core.frame.DataFrame = pd.concat([games_sub1, games_sub2])\n",
    "\n",
    "# Intitial Assessment (info, memory usage, shape, and head)\n",
    "print(\"=\"*20 + \" DataFrame Information \" + \"=\"*20)\n",
    "games_raw.info()\n",
    "print(\"=\"*20 + \" DataFrame Information \" + \"=\"*20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" Memory Usage \" + \"=\"*20)\n",
    "print(f\"{games_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"=\"*20 + \" Memory Usage \" + \"=\"*20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" DataFrame Shape \" + \"=\"*20)\n",
    "print(games_raw.shape)\n",
    "print(\"=\"*20 + \" DataFrame Shape \" + \"=\"*20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" DataFrame Head \" + \"=\"*20)\n",
    "print(games_raw.head())\n",
    "print(\"=\"*20 + \" DataFrame Head \" + \"=\"*20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Section 2: Data Quality Assessment\n",
    "In this section, the data values are examined to inform cleaning decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of NA values in each column\n",
    "print(\"\\n\" + \"=\"*20 + \" NA Values\" + \"=\"*20)\n",
    "print(games_raw.isna().sum())\n",
    "print(\"=\"*20 + \" NA Values \" + \"=\"*20)\n",
    "\n",
    "# Find the number of unique values in each column\n",
    "print(\"\\n\" + \"=\"*20 + \" Unique Values \" + \"=\"*20)\n",
    "print(games_raw.nunique())\n",
    "print(\"=\"*20 + \" Unique Values \" + \"=\"*20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Section 3: Cleaning Decisions\n",
    "In this section, the DataFrame is cleaned based on the analysis of the previous section, as well as the return of the `.head()` method in Section 1. Section 1 is used to inform type casting decisions, and Section 2 is used to provide early warning of type casting errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Drop Unwanted Columns:\n",
    "Columns that do not contribute to analysis of the dataset or aid in answering the question are dropped from the DataFrame in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unneeded columns from the dataframe using the .drop() method\n",
    "games = games_raw.drop(columns=[\"required_age\",\n",
    "                              \"dlc_count\",\n",
    "                              \"detailed_description\", \n",
    "                              \"about_the_game\", \n",
    "                              \"short_description\", \n",
    "                              \"reviews\", \n",
    "                              \"support_url\", \n",
    "                              \"support_email\", \n",
    "                              \"estimated_owners\",\n",
    "                              \"metacritic_score\", \n",
    "                              \"metacritic_url\", \n",
    "                              \"achievements\", \n",
    "                              \"recommendations\", \n",
    "                              \"notes\",\n",
    "                              \"full_audio_languages\",\n",
    "                              \"packages\",\n",
    "                              \"categories\",  \n",
    "                              \"screenshots\", \n",
    "                              \"movies\",\n",
    "                              \"user_score\", \n",
    "                              \"score_rank\", \n",
    "                              \"tags\",\n",
    "                              \"pct_pos_total\",\n",
    "                              \"pct_pos_recent\",\n",
    "                              \"average_playtime_forever\", \n",
    "                              \"average_playtime_2weeks\", \n",
    "                              \"median_playtime_forever\",\n",
    "                              \"median_playtime_2weeks\", \n",
    "                              \"header_image\", \n",
    "                              \"website\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Set the Index:\n",
    "In Section 2, it was found that the column \"AppID\" has _nearly_ the same number of unique values (83653) as the number of rows (83655), making this a great index option. Furtheremore, this column has 0 NA values. For these reasons, \"AppID\" was selected as the index. Some values were found with clear encoding errors, these were scrapped in the process, as all columns in those rows were improperly encoded, and thus unusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index:\n",
    "# Cast the \"AppID\" column to numeric (NA if not numeric-like) and drop rows with NA values\n",
    "games['AppID'] = pd.to_numeric(games['AppID'], downcast='integer', errors='coerce')\n",
    "games = games.dropna(subset=[\"AppID\"])\n",
    "\n",
    "# Convert remaining rows' \"AppID\" value to uint32 and then set the index of the DataFrame to this column\n",
    "games[\"AppID\"] = games['AppID'].astype('uint32')\n",
    "\n",
    "# Set the data frame index to the \"AppID\" column\n",
    "games = games.set_index(\"AppID\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Cast Column Data Types:\n",
    "Three main types of columns are converted below:\n",
    "1. Straight-forward string and numeric columns. These are converted to the most appropriate type using `.astype()` with a mapping of column:type pairs as the argument.\n",
    "2. Columns that are \"list-like\" (e.g. \\['English', 'Vietnamese'\\]). These values are modified to be forward slash seperated for subsequent analysis (e.g. English/Vietnamese). Language columns are converted to strings, while genre, developer, and publisher columns are converted to categories due to a high count of repeat values as determined in Section 2.\n",
    "3. Boolean values. The dataset uses \"TRUE\" and \"FALSE\" for its boolean values, which `.astype()` always interprets as True. To solve this problem, each of these columns are initially cast as string, and then are set equal to a boolean mask on the condition `df['col'] == \"TRUE\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert straightforward numeric and string column data types\n",
    "games = games.astype({'name' : 'string',\n",
    "                      'release_date' : 'datetime64[ns]',\n",
    "                      'price' : 'float32',\n",
    "                      'windows' : 'string',\n",
    "                      'mac' : 'string',\n",
    "                      'linux' : 'string',\n",
    "                      'positive' : 'float64',\n",
    "                      'negative' : 'float64',\n",
    "                      'peak_ccu' : 'Int64',\n",
    "                      'num_reviews_recent' : 'Int64'\n",
    "                   })\n",
    "\n",
    "# Convert the \"list-like\" columns from ['thing1','thing2'] string to \"thing1/thing2\" category or string\n",
    "games[\"supported_languages\"] = unpack_list_like(games[\"supported_languages\"], asType='string')\n",
    "games['developers'] = unpack_list_like(games[\"developers\"], asType='string')\n",
    "games['publishers'] = unpack_list_like(games[\"publishers\"], asType='string')\n",
    "games['genres'] = unpack_list_like(games[\"genres\"], asType='category')\n",
    "\n",
    "# Set incompatible boolean value columns equal to a bool mask to map TRUE to True and FALSE to False\n",
    "games['windows'] = games['windows'].str.strip() == \"TRUE\"\n",
    "games['mac'] = games['mac'].str.strip() == \"TRUE\"\n",
    "games['linux'] = games['linux'].str.strip() == \"TRUE\"\n",
    "\n",
    "# Report cleaned DataFrame size\n",
    "print(f\"The size of the cleaned DataFrame is {games.memory_usage(deep=True).sum() / 1024**2:.2f}MB\")\n",
    "\n",
    "# Find the number of unique values in each column\n",
    "print(\"\\n\" + \"=\"*20 + \" Unique Values (Cleaned) \" + \"=\"*20)\n",
    "print(games.nunique())\n",
    "print(\"=\"*20 + \" Unique Values (Cleaned) \" + \"=\"*20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Section 4: Statistical EDA\n",
    "In this section's subsections, several variables/groups of variables are characterized using statistical measurement and visualization transformations. Performing statistical and visual operations on these values allows their distributions to be understood, which provides insight into the measures and their assocaited values. First, individual features are analyzed, and then relations between various features are explored."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Release Date EDA:\n",
    "The goal of this section is to characterize the `release_date` column statistically and visually to understand how game release frequencies have changed over time. After cleaning this dataset, 83646 valid observations remain. Section 2 revealed that there are only 4503 unique release dates. As such, it becomes evident that a frequency analysis can provide some insight into the frequency distribution of game release dates. Since there are 4503 unique values of day/month/year, Pandas' built-in plotting struggles to handle axis labels, and as a result, these values were temporarily reduced to a year only value, as this alone is sufficient to understand the change in released game counts over time. Note: Logarithmic scale is used for the number of released games (y-axis) to esnure an insightful bar is plotted for early years (pre-2006) with low release counts. In addition to this frequency analysis, the average number of released games in a given year is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new series of the release dates, with dates reduced to year only.\n",
    "release_year_freq = (games['release_date']\n",
    "                     .dt.year\n",
    "                     .value_counts())\n",
    "\n",
    "# Plot the release year frequency using Pandas\n",
    "release_year_freq.sort_index().plot(kind='bar', \n",
    "                                    title=\"Release Year Frequency\", \n",
    "                                    logy=True, xlabel='Release Year', \n",
    "                                    ylabel='Number of Games Released')\n",
    "plt.show()\n",
    "\n",
    "# Show the frequency of game releases, sorted by number of releases\n",
    "print(\"\\n Sorted Game Release Frequency by Year\")\n",
    "print(release_year_freq)\n",
    "\n",
    "# Statistically characterize the release year distribution\n",
    "print(f\"\\nThe average year has approximately {release_year_freq.mean():.1f} games released.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Game Price EDA:\n",
    "The goal of this section is to characterize the price distribution of the games statistically and visually, across the dataset. Here, Pandas' `.describe()` method is used to statistically characterize the distribution of the `price` continuous variable. Furthermore, a logarithmic plot is provided to understand the _entire_ distribution due to the existance of a handful of games in the 975-1000 USD range. Additionally, a histogram is provided in the 0-75 USD range to characterize the _heavy_ majority of the distribution, as shown by the logarithmic plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistically descibe the distribution of the price column\n",
    "print(games['price'].describe())\n",
    "\n",
    "# Plot the entire frequency of prices using a histogram, syncing bins with xticks\n",
    "bins_xticks_range = range(0, 1001, 25)\n",
    "games['price'].plot(kind='hist', \n",
    "                    title=\"Full Logarithmic Frequency Distribution of Game Price\", \n",
    "                    logy=True, \n",
    "                    xlabel=\"Price in USD($)\", \n",
    "                    bins=bins_xticks_range, \n",
    "                    xticks=bins_xticks_range, \n",
    "                    rot=90\n",
    "                   )\n",
    "plt.show()\n",
    "\n",
    "# Plot the reduced frequency of prices using a histogram, syncing bins with xticks\n",
    "bins_xticks_range_reduced = range(0, 80, 5)\n",
    "games['price'].plot(kind='hist', \n",
    "                    title=\"Reduced ($0-75 USD) Frequency Distribution of Game Price\",  \n",
    "                    xlabel=\"Price in USD($)\", \n",
    "                    bins=bins_xticks_range_reduced, \n",
    "                    xticks=bins_xticks_range_reduced, \n",
    "                    rot=90,\n",
    "                    xlim=(0,75)\n",
    "                   )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Operating System Offering EDA:\n",
    "In this section, the operating system offerings of the games in the dataset are analyzed. The counts of games offered on each OS is reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the counts of each operating systems' games\n",
    "print(f\"Windows has {games['windows'].sum()} games available.\")\n",
    "print(f\"Macintosh has {games['mac'].sum()} games available.\")\n",
    "print(f\"Linux has {games['linux'].sum()} games available.\")\n",
    "\n",
    "games[['windows', 'mac', 'linux']].sum().plot(kind='bar',\n",
    "                                              title=\"Number of Games per OS\",\n",
    "                                              xlabel='Operating System',\n",
    "                                              ylabel='Number of Games Offered',\n",
    "                                              rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### User Review (Positive/Negative) EDA:\n",
    "In this section, the number of user reviews (both positive and negative) are statistically analyzed. Additionally, positive and negative reviews are plotted in a single figure to offer a side-by side comparison of the two measures. Note here that the use of the `.describe()` method is primarily used to inform plot parameter selection, and the key takeaways of the statistical measures of these features is restated after the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistically characterize the number of positive reviews\n",
    "print(\"Positive Review Statistics:\")\n",
    "print(games['positive'].describe().apply(lambda x: format(x, '.2f')))\n",
    "\n",
    "# Statistically characterize the number of negative reviews\n",
    "print(\"\\nNegative Review Statistics:\")\n",
    "print(games['negative'].describe().apply(lambda x: format(x, '.2f')))\n",
    "\n",
    "# Plot the histograms of positive and negative reviews in a single figure\n",
    "games.plot(kind='scatter', \n",
    "           x='positive', \n",
    "           y='negative',\n",
    "           xlabel='Positive Reviews',\n",
    "           ylabel='Negative Reviews',\n",
    "           title='Positive vs. Negative Reviews'\n",
    "          )\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Bivariate Analysis 1: Average Game Price per Release Year\n",
    "In this section, the average price of games was compared to the release year using `.groupby()` with the `.mean()` aggregation function. Results were plotted as a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column containing the release year as a category\n",
    "games['release_year'] = (games['release_date'].dt.year).astype('category')\n",
    "\n",
    "# Group by year, aggregate price average\n",
    "avg_price_by_year = (games.groupby('release_year', observed=False)['price']\n",
    "                     .mean()\n",
    "                     .sort_index())\n",
    "\n",
    "# Report average price per year\n",
    "print(avg_price_by_year)\n",
    "\n",
    "# Plot the results using a bar graph\n",
    "avg_price_by_year.plot(kind='bar', \n",
    "                       title=\"Average Yearly Game Price\", \n",
    "                       xlabel='Release Year', \n",
    "                       ylabel='Average Game Price (USD)',\n",
    "                       rot=90.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Bivariate Analysis 2: Operating System Game Releases by Year\n",
    "In this section, the number of released games per operating system were computed for each release year using the `.groupby()` methd with the `.sum()` aggregation function to count results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year, aggregate count of each OS game releases\n",
    "os_releases_by_year = (games.groupby('release_year', observed=False)[['windows', 'mac', 'linux']]\n",
    "                     .sum()\n",
    "                     .sort_index())\n",
    "\n",
    "# Report average price per year, flattened\n",
    "print(os_releases_by_year.reset_index())\n",
    "\n",
    "os_releases_by_year.plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    figsize=(14, 6),\n",
    "    xlabel=\"Release Year\",\n",
    "    ylabel=\"Number of Games Released\",\n",
    "    rot=90.0,\n",
    "    title=\"Number of Game Releases of each OS per Year\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1e6b7-34c6-4519-aa48-a5f043182a01",
   "metadata": {},
   "source": [
    "### Section 5: Transform\n",
    "In this section, additional features were engineered to assist in the analysis. The objective was to analyze how game reviews and game peak concurrent users (peak_ccu) have changed over time. Furthermore, an analysis was conducted to see how language support affects userbase, price, and reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a23f97-7bb7-4d70-a124-362a32105a5a",
   "metadata": {},
   "source": [
    "### Percent Positive Reveiws Transform\n",
    "In this section, the percent positive review was calculated and grouped by year using the `.groupby()` method in conjunction with the `.mean()` aggregation. Furthermore, a boxplot is generated to aid in the analysis of the distribution over time. The objective for this analysis is to characterize the change in review positivity over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create percentage positive column\n",
    "games[\"percent_positive\"] = games[\"positive\"] / (games[\"positive\"] + games[\"negative\"])\n",
    "\n",
    "# Report average number of positive and negative reviews and average percentage positive\n",
    "rec_year = games.groupby(\"release_year\", observed=False)[[\"positive\", \"negative\", \"percent_positive\"]].mean()\n",
    "print(rec_year)\n",
    "\n",
    "# Plot average percent positive review by release year\n",
    "ax = games.plot(kind='box',\n",
    "                column='percent_positive', \n",
    "                by='release_year',\n",
    "                rot=90.0, \n",
    "                title=\"Distribution of Percent Positive Reviews by Release Year\", \n",
    "                xlabel=\"Release Year\", \n",
    "                ylabel=\"Positive Reviews (%)\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e4ea9-e860-4ac4-84bf-a07c9fd90e47",
   "metadata": {},
   "source": [
    "### Peak Concurrent User (Peak_CCU) Transformation\n",
    "In this section, the mean, sum, and max peak concurrent user count for each game release year was computed and analyzed. The objective of this analysis is to characterize the peak_ccu change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby peak ccu to get mean, sum and count for each year\n",
    "year_pccu = games.groupby(\"release_year\", observed=False)[[\"peak_ccu\",\"name\"]].agg(\n",
    "    {\"peak_ccu\" : [\"mean\", \"sum\", \"max\"],\n",
    "     \"name\" : \"count\"})\n",
    "print(year_pccu)\n",
    "\n",
    "# peak_ccu statistical analysis\n",
    "print(f'\\nPeak CCU Stats:\\n{games[\"peak_ccu\"].describe()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93107d88-785a-423b-a20d-9efacc542ac5",
   "metadata": {},
   "source": [
    "### Language Support Transformation\n",
    "In this section, 7 columns are added that indicate via a boolean value whether or not a given game is offered in that column's associated language. The seven languages analyzed are English, Chinese, Japanese, Spanish, German, French, and Russian. Each of these language tests include use of dialects (e.g. Chinese Mandarin and Chinese Traditional both evaluate to True for the `Chinese` column). Comparison of these results is conducted by plotting like-metrics against one another for each language to characterize language support effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3218a-79c5-45b2-abc7-980273be8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the new columns for each language of interest.\n",
    "games[\"English\"] = games[\"supported_languages\"].str.contains(\"English\")\n",
    "games[\"Chinese\"] = games[\"supported_languages\"].str.contains(\"Chinese\")\n",
    "games[\"Japanese\"] = games[\"supported_languages\"].str.contains(\"Japanese\")\n",
    "games[\"Spanish\"] = games[\"supported_languages\"].str.contains(\"Spanish\")\n",
    "games[\"German\"] = games[\"supported_languages\"].str.contains(\"German\")\n",
    "games[\"French\"] = games[\"supported_languages\"].str.contains(\"French\")\n",
    "games[\"Russian\"] = games[\"supported_languages\"].str.contains(\"Russian\")\n",
    "\n",
    "# For each of the languages of interest, compute number of games supported, average game price, average peak_ccu, average positive reviews, and average negative reviews\n",
    "lang = [\"English\",\"Chinese\", \"Japanese\", \"Spanish\", \"German\", \"French\", \"Russian\"]\n",
    "for x in lang:\n",
    "    print(f'Language: {x}')\n",
    "    print(f'Total {x} games: {games[x].sum()}')\n",
    "    print(f'Average {x} games price: {games[\"price\"].loc[games[x] == True].mean():.2f}')\n",
    "    print(f'Average {x} games peak ccu: {games[\"peak_ccu\"].loc[games[x] == True].mean():.2f}')\n",
    "    print(f'Average {x} games positive reviews: {games[\"positive\"].loc[games[x] == True].mean():.2f}')\n",
    "    print(f'Average {x} games negative reviews: {games[\"negative\"].loc[games[x] == True].mean():.2f}\\n')\n",
    "\n",
    "# Plot results in a figure to compare each metric for each language (using matplotlib to customize layout)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"Language Support Effects Analysis\", fontsize=12)\n",
    "\n",
    "# Plot game counts by language\n",
    "lang_counts = games[lang].sum()\n",
    "axes[0, 0].bar(x=lang, height=lang_counts)\n",
    "axes[0, 0].set_xticks(range(len(lang)))\n",
    "axes[0, 0].set_xticklabels(labels=lang, rotation=90.0)\n",
    "axes[0, 0].set_title(\"Number of Games Supported\")\n",
    "axes[0, 0].set_ylabel(\"Number of Games\")\n",
    "\n",
    "# Plot average price by language\n",
    "average_prices = [games[\"price\"].loc[games[x] == True].mean() for x in lang]\n",
    "axes[0, 1].bar(x=lang, height=average_prices)\n",
    "axes[0, 1].set_xticks(range(len(lang)))\n",
    "axes[0, 1].set_xticklabels(labels=lang, rotation=90.0)\n",
    "axes[0, 1].set_title(\"Average Price\")\n",
    "axes[0, 1].set_ylabel(\"Price (USD)\")\n",
    "\n",
    "# Plot average peak_ccu by language\n",
    "average_ccu = [games[\"peak_ccu\"].loc[games[x] == True].mean() for x in lang]\n",
    "axes[1, 0].bar(x=lang, height=average_ccu)\n",
    "axes[1, 0].set_xticks(range(len(lang)))\n",
    "axes[1, 0].set_xticklabels(labels=lang, rotation=90.0)\n",
    "axes[1, 0].set_title(\"Average Peak CCU\")\n",
    "axes[1, 0].set_ylabel(\"Number of Games\")\n",
    "\n",
    "# Plot average percent positive review by language\n",
    "average_pospct = [games[\"percent_positive\"].loc[games[x] == True].mean() for x in lang]\n",
    "axes[1, 1].bar(x=lang, height=average_pospct)\n",
    "axes[1, 1].set_xticks(range(len(lang)))\n",
    "axes[1, 1].set_xticklabels(labels=lang, rotation=90.0)\n",
    "axes[1, 1].set_title(\"Average % Positive Review\")\n",
    "axes[1, 1].set_ylabel(\"Number of Games\")\n",
    "\n",
    "# Set tight layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bd4e4-8bc1-4317-8fe5-a0e1945df3f9",
   "metadata": {},
   "source": [
    "### Developer/Publisher Transformation\n",
    "In this section, the developer and publisher of each game was analyzed. A feature was engineered that determines whether or not a publisher is the same as the developer. The objective of this analysis is to determine whether or not price is affected by having a different publishing and developing company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80495958-a247-4def-8a20-b4f84369e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the developer == publisher, clean NaN values to None for error handling\n",
    "games[\"developers\"] = games[\"developers\"].fillna(\"None\")\n",
    "games[\"publishers\"] = games[\"publishers\"].fillna(\"None\")\n",
    "games[\"dev_pub\"] = games.apply(lambda x: None if x[\"developers\"] == \"None\" or x[\"publishers\"] == \"None\" else x[\"developers\"] in x[\"publishers\"], axis = 1)\n",
    "\n",
    "\n",
    "group_price = games.groupby(\"dev_pub\")[[\"price\", \"name\"]].agg({\n",
    "    \"price\" : [\"mean\", \"sum\"],\n",
    "    \"name\" : \"count\"})\n",
    "\n",
    "print(group_price)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Section 6: Save and Document\n",
    "In this section, the resulting cleaned dataset with additional engineered features is exported as pickle (for Python users) and CSV (for compatability). Note that these files are ignored by Git for tracking since Git is for source code and not exports."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Exporting the Cleaned and Feature Engineered Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the dataset to export the AppID along with the rest of the data.\n",
    "games.reset_index()\n",
    "\n",
    "# Export as a Pickle file\n",
    "games.to_pickle(\"export/games_cleaned_added_features.pkl\")\n",
    "\n",
    "# Export as CSV, removing index since after resetting the index, the index is a simple integer value\n",
    "games.to_csv(\"export/games_cleaned_added_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e11327-5861-4c0b-b47c-df214ef416fc",
   "metadata": {},
   "source": [
    "### Analsysis Documentation:\n",
    "In this section, each section above (Sections 1-5) is recapped and findings and insights gleaned from analysis sections are discussed.\n",
    "\n",
    "1. In Section 1, the data was loaded as default types and initially assessed using a variety of available built-in Pandas methods to examine the data available. Methods used to initially characterize the data are `.info()`, `.memory_usage()`, `.shape`, and `.head()`. The examination of the values returned from these operations was used to inform the Data Quality Assessment in Section 2, as well as the Cleaning Decisions made in Section 3. The use of these methods on the initial CSV data load was critical in characterizing the raw state of the data as it exists in the CSV.\n",
    "2. In Section 2, an intial quality assessment of the data was performed. This assessment differed from the assessment in Section 1 because while the Section 1 assessment was used to inform type decisions, memory management decisions, and order of observation decisions, the assessment in Section 2 assessed the dataset for value completeness, and repeat values. The assessment in Section 2 also played a role in type decisions and memeory management, particularly when it came to opportunities for categorical or boolean data types. Furthermore, the assessment in Section 2 was critical in assessing each column's suitability to perform a role as the index of the dataframe by examining missing values for each column, as well as the number of unique values.\n",
    "3. In Section 3, the Assessments in Section 1 and 2 were used as an informative basis on which to decide which columns should be cleaned in which way as which types. This assisted the cleaning process by providing visability into the columns available, their inital type, number of NA values, and number of unique values. From those findings, the unique AppID was selected as the index. Additionally from the return of `.head()` in Section 1, the decision was made on which columns to drop altogether in Section 3 using the `.drop()` method. The return of `.head()` was also used to make informed type decisions on each column that was retained. The assessment in Section 1 revealed four columns that act as \"list-like\" string objects (e.g. \"\\[\"Thing1\", \"Thing2\"\\]\"), which were \"unpacked\" and reformatted in Section 3 using a user-defined function. This user-defined function allowed for these types of columns to be operated on as true Python lists much easier. Additionally, the assessment in Section 1 and 2 identified three columns with boolean-like values, that were converted to true boolean values in Section 3. Following the execution of these cleaning operations, the number of new unique values was reported, as well as the new memory usage of the cleaned dataframe. The initial memory usage was 572.63MB and the cleaned memory usage is 32.32MB.\n",
    "4. In Section 4, statistical EDA was performed on several univariate columns. Additionally, two bivariate analyses were performed to examine parameter relations as it pertains to the games. First, count of games released in each release year was computed and plotted. A logarithmic scale was used due to the exponential nature in which game releases per year grew. It was found that game releases increase significantly year-over-year. 2023 had the most game releases (Note: Dataset goes until 05-2024, so 2024 is not completetly captured). Next, the statistical distribution of game price was computed. The average game (all-time) was found to be about 7.50 USD, with a standard deviation of 13.10 USD. A historgram was used to visually convey the distribution of games in defined price bins. Another histogram, with a reduced price range was provided as well to avoid use of the logarithmic y-axis. It was found that the vast majority of games are free, and that the maximum game price is 999.97 USD. Operating System support was also examined, by the number of games availble with each OS type. It was found that Windows has, by far, the most games available. A barplot was provided with this information. Next, user reviews (both positive and negative) were examined. The `.describe()` method was used to statistically characterize the paired positive and negative review values. This information was also provided in the form of a scatter plot with x values as positive reviews and y values as negative reviews. A point was plotted on this scatter plot for each game in the dataset. It was found that the average game has 1267 positive reviews, and 207 negative reviews. A bivariate analysis was then conducted on the game price average for each year. It was found that average game price fluctuates heavily year-over-year, with 2002 being the most expensive year for games at an average price of 14.99 USD. The cheapest year for games was 1999 with an average price of 4.99 USD. This information was provided visually in a barplot format as well. The second bivarite analysis examined OS count and release year to characterize how OS releases have changed over time. It was found that new releases for each operating system roughly track together, likely indicating that most Mac and Linux games are also offered on Windows. This information was visually provided in the form of a stacked bar chart.\n",
    "5. In Section 5, four features were engineered to further characterize the dataset. First, a feature called `percent_positive` was engineered that examined the percentage of positive reviews for each game. This was compared on a release year basis and, due to complexity of the distributions year to year, was reported visually as a boxplot for each year's precent positive distribution. The second feature engineered was a yearly examination of `peak_ccu` (peak concurrent users). In this examined relation, `.describe()` was used to statistically describe the distribution obtained by grouping the peak_ccu statistics by `release_year` and aggregating count, mean, and max. Next, supported language was examined using seven engineered features, one for each language of interest. The seven languages analyzed are English, Chinese, Japanese, Spanish, German, French, and Russian. For each language, a boolean mask was created for the new column that determined if the language assocaited with the column was supported by a given game. Each language column had the associated game count, average price, peak concurrent user count, and positive/negative review count analyzed. This information was printed textually for each language and organized into a 2x2 subplot figure as well. Through this analysis, it was found that English supports the most games by a significant margin, and also has the lowest average game price, but has the lowest average peak concurrent user count. Conversely, Japanese has the fewest supported games of the languages analyzed, and has the highest average game price, and the highest average peak concurrent user count. The inversely proportional relation between peak_ccu and supported game count is likely due to smaller games only supporting English, and having a smaller overall userbase, while larger games from larger studios likely support more languages, and naturally garner more users. Further analysis would be needed to confirm this hypothesis. Interestingly, Japanese supported games have the highest percent positive reviews, while English supported games have the lowest. Again, this liekly points to the amount of resources available to smaller studios. Finally, a feature called `developer_publisher` was created that examines whether or not the publishing company is the same as the devloping company for each game. The hypothesis was formed that games with different developers and publishers liekly have higher prices due to overhead costs needed to market a game developed by another company. This hypothesis was confirmed by the analysis as the average game developed and published by the same company is 6.71 USD, while games with differing publishers and developers averaged 9.42 USD. This represents a difference in average price of 2.71 USD. Additionally, it was found that far more games are developed and published by the same company (59495 vs. 24110)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insy6500)",
   "language": "python",
   "name": "insy6500"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
